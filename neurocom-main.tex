
%% 
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%% SP 2008/03/01

\documentclass[preprint,12pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{algorithm}
\usepackage{algorithmic}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}
%\usepackage{graphicx}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}

\usepackage{color}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Neucocomputing}

%%\input{AllCommands}
%%\input{AllNewCommands}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Binary feedback-based online multiclass learning}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author[centrale,lif]{Hongliang Zhong}
\author[centrale,ins]{Emmanuel Dauc\'e \corref{cor1}}

\address[centrale]{Ecole Centrale de Marseille}
\address[lif]{Laboratoire d'informatique Fondamentale}
\address[ins]{Institut de Neurosciences des Syst\`emes}

\cortext[cor1]{Corresponding author}

\begin{abstract}
%% Text of abstract

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Mot1 \sep Mot2
%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{sec:lentete}


Categorical online learning is a well-documented class of learning problems in which ($i$) a time order is defined on the sequence of input examples  $x_1$, ..., $x_t$, ... with the classifier update taking place after each example presentation and ($ii$) the output space is categorical, i.e. a single response $y_t$, among $K>1$ possibilities, is expected at trial $t$. The multiclass learning task typically addresses object recognition such as OCR, face recognition, speech recognition etc. In contrast with the offline approach, online learning takes the form of an iterative process, relying on a time-ordered sequence of observations, queries and feedbacks.   
Two well-known setups obey to this class, i.e. the contextual bandit setup (see  \cite{lai1985asymptotically} and \cite{auer2002finite}) and supervised online learning setup (see  \cite{rosenblatt1958perceptron}, \cite{duda1973pattern} and \cite{freund1999large}), the two setups mainly differing by the nature of the feedback. Whether abundant or scarce, deterministic or stochastic, stationary or adversarial, the feedback characteristics critically shape the design of the learning algorithms.  

The case we address in the following is the one-bit feedback in multiclass classification tasks, occasionally called the bandit feedback (see \cite{kakade2008efficient}). This case lies at the crossroad of the supervised online learning setup and the bandit setup.  A one-bit feedback reflects a ``hit-or-miss'' learning situations, in which a single bit indicates to the learner whether its categorical response was correct (``hit'') or incorrect (``miss''). Wether simple in its principle, the problem is specific to the online approach and was only recently identified and analyzed by the community (see \cite{kakade2008efficient}, \cite{gentile2012multilabel}, etc.). 
Apart from the original work of \cite{kakade2008efficient}, most recent approaches to this problem are based on regularized linear models (see \cite{li2010contextual}, \cite{crammer2013multiclass}, \cite{ngo2013upper}), to which the powerful methods of contextual bandit, including UCB-like non-stochastic exploration strategy (see \cite{lai1985asymptotically}), do apply. While providing almost optimal convergence rates, model-based approaches suffer from a quadratic complexity in space that limits their applicability to large-dimensional datasets.  On contrary, discriminant-based approaches to online classification (see \cite{anlauf1989adatron}, \cite{crammer2006online}) only provide upper bounds on the error rate, but display linear scaling in size with smaller memory footprint. 

We adapt in this paper the quadratic optimization approach proposed by \cite{crammer2006online} to the one-bit feedback case. Online learning relies on a local norm minimization under standard linear-convex constraints. Our conservative approach allows to determine similar regret bounds than in the original paper of \cite{crammer2006online}, providing strong garanties on  convergence capabilities, that are confronted to the other methods over synthetic and real datasets.  

\section{Outlook}

\paragraph{Online learning} Online learning is a \textit{process} in which an agent (the ``learner'') probes its environment sequentially to obtain \textit{information} that is eventually used to improve its \textit{fitness}.
The universe being initially hidden, the online approach works on probing the database (universe) through individual queries that step-by-step unveil the environment. 

The sequential organization of learning is present in the most traditional setups such as the Bandit problems (see \cite{robbins1952bandit}) and in the Perceptron algorithm (see \cite{rosenblatt1958perceptron}). The general setup we consider here is the case of an ``open-loop'' \textit{actionnable} universe. The time-indexed observations $x_1$, ..., $x_t$, ... are independent (causally disconnected). Each observation causes the learner to output a single action $\tilde{y}_t$ out of $K > 1$ possible actions. Every action provides a feedback $f_t$ that is an \textit{information} about a \textit{value of interest} (or reward) $r_t$ that needs to be maximized over time. The feedback is \textit{explicit} in most contextual bandit setups, i.e. $f_t=r_t$ (see \cite{lai1985asymptotically} and \cite{auer2002finite}) while it takes the form of a category $f_t=y_t$ in traditional online learning setups, that \textit{indirectly} provides a quantity to maximise through a loss function $r(x_t, y_t) = -l(x_t,\tilde{y}_t, y_t)$ (see  \cite{duda1973pattern}, \cite{freund1999large}, \cite{kivinen2004online}, \cite{crammer2006online}). Solving the problem thus means both  learning the universe from experience and optimizing the final reward. 

\paragraph{Linear models}
Multiclass classification requires  finding an appropriate class $y \in \{1,... K\}$ for every observation vector $x \in \mathbb{R}^d$.
The \textit{decision function} (or politics) $\tilde{y}_t \sim h(x_t)$ embodies both prior information about the regularity of the universe and experience-based information. A reasonable assumption  is to consider that  close-by contexts should provide close-by rewards distributions. 
The linear approach to classification means to separate the input space in regions, where the different regions are defined by a set of linear mappings. We consider in that case a  classifier $W = (w^1,..,w^k) \in \mathbb{R}^{K \times d}$ so that, for every categorical response $k$, a \emph{similarity score} $\langle w^k, x_t\rangle$ is carried out. 
%defined as a set of $K$ linear mappings. 
The linear assumption allows ($i$) to 
settle choice (or action) over class similarity prediction and ($ii$) to update models over prediction accuracy.
% distributions of rewards biased by a context $x$ that is given to the gambler before decision making. The universe is now made of couples (context, rewards distribution). 
%Two principal approaches are identified in the supervised setting. 

The set of linear mappings can either be considered class-prototypes or class-separatrices. The first case implements a \textit{model-based} approach, and the second a \textit{discriminant-based} approach. In the first case,  second order gradient descent methods such as natural gradient (see \cite{amari2000adaptive}), Gauss-Newton (see \cite{le2004large}), and second order perceptron (see \cite{cesa2005second}) provide almost optimal convergence rates while scaling quadratically with exemplar dimension.
%Sequential gradient descent/Markov chain approach to learning is now commonplace in supervised learning for its prominent regularization (see \cite{le2004large}) and auto-encoding 
%properties, powerfully exploited in the RBM approach (see \cite{hinton2006fast}).
In the second case, the quadratic optimization approach (see \cite{anlauf1989adatron}, \cite{crammer2006online}) only provide upper bounds on the error rate but display linear scaling in size, label-unbalance robustness and more generally provide a smaller memory footprint. 


\paragraph{One-bit feedback}
The one-bit feedback is a specific online learning setup that implements, in a principled way, the scarce labelling information problem. After reading $\tilde{y}_t$, instead of providing a label, the universe provides a single bit of information called the \textit{label hit}, i.e. $f_t = 1$ if  $\tilde{y}_t=y_t$ and $f_t = 0$ elsewhere. The objective, just as in the supervised case, is to avoid label misses and maximise labels hits. A specific update function needs to be defined to allow label hit improvement over time. 




This problem was coined the ``bandit feedback classification problem'' by \cite{kakade2008efficient}. Taking inspiration from the multiclass perceptron (\cite{duda1973pattern}), a time-effective algorithm, called the ``Banditron'', is established:
$K$ linear mappings $w^1, ..., w^K$ are defined and class-decision relies on a best-match politics, i.e.
 $$\hat{y} = \arg \max_{k \in\{1,..,K\}}  \langle w^k, x \rangle$$
Contrarily to the supervised case, an \textit{exploration} politics needs to be defined to efficiently sample the decision space. The Banditron adopts a simple  $\varepsilon-greedy$ search allowing the non-maximal responses to be occasionally chosen in a uniform way:
 $$P(\tilde{Y}=k) = (1-\varepsilon) \delta(k,\hat{y}) + \frac{\varepsilon}{K}$$ with $\varepsilon \in [0,1]$.
Given an initial set of $K$ null-vector hyperplanes $W_0 = (w^{1}_0$, ..., $w^{K}_0)$, the update at time $t$ is ~:
$$ W_t = W_{t-1} + \frac{\delta(\tilde{y}_t ,y_t) X_t^{\tilde{y}_t}}{P(\tilde{Y}=\tilde{y}_t)} - X_t^{\hat{y}_t}$$   
with the convention $X_t^k = (0, ..., 0,  x_t, 0, .., 0)$ a sequence of null vectors, except at position $k$ where the observation vector $x_t$ is found, so that $\langle W, X^k\rangle = \langle w^k, x\rangle$. The expectation of the update is shown to be that of the multiclass perceptron, and a convergence to the correponding perceptron classifier is obtained, with a finite bound on the cumulative error in linearly separable cases. 

{\color{blue}
Toujours dans le cadre des classifieurs linéaires, plusieurs algorithmes d'apprentissage de bandits contextuels ont été proposés dans la littérature. Dans nos expérimentations numériques, nous considérerons également ici l'algorithme ``Confidit'' proposé par Crammer et Gentile \cite{crammer2013multiclass}, reposant sur le perceptron d'ordre 2 et l'exploration non stochastique basée sur le principe UCB \cite{lai1985asymptotically}, et présentant des profils de convergence plus favorables que le Banditron. }


\paragraph{Online risk minimization}
Under the discriminant approach to linear classification, the separating hyperplanes are expected to optimally separate the input space according to the misclassification risk. This risk can for instance be minimized through a margin principle (see \cite{vapnik1998statistical}), imposing a non-zero distance of known class exemplars to the classification boundaries. 

%In contrast with the \textit{offline} setting, where the classifiers are obtained through quadratic optimization over a finite set of classified exemples (see \cite{vapnik1998statistical}), no predefinite database is considered under the online setting. 
%The observation vectors arrive sequentially according to a temporal index $t$ (temporal order). 
%When facing a new observation vector $x_t$, the learner emits a categorical response $\tilde{y}_t \in \{1,... K\}$ and then obtains a feedback $f_t$. The feedback provides an indication about the correctness of the learner's choice. 
Following   \cite{kivinen2004online} and \cite{crammer2006online}, we consider a sequential approach to risk minimization.
%, relying on a specific sequence of observations, decisions and feedbacks.
%In the supervised setting, the feedback is equal to the expected response $f_t = y_t \in \{1,... K\}$. In the non-supervised setting, no feedback is provided.
%Our objective here is to provide a way to extend those local online optimization approaches  to the \emph{binary feedback} case, where the feedback is composed of a single bit of information $f_t \in\{0,1\}$, i.e. $f_t = 1$ if the class proposed by the clasifier is correct and 0 otherwise. This setup may provide avenues toward the more general contextual bandit and  reinforcement learning setups.
%given an objective (or loss) function considered defined over the full input space.
%A feedback is a \emph{local} measure of  learning achievement (or failure), 
The classifier is updated through local measures of a loss function $l_t = l(x_t,\tilde{y}_t,f_t,W_t)$. Contrarily to a mere reward (or penalty), a loss function comes with an implicit set point, namely $l_t=0$, that grants response correctness under a margin constraint. In \cite{kivinen2004online}, a loss-minimizer gradient descent update is combined with a norm-minimizer on the decision function, while \cite{crammer2006online} adopt a quadratic norm-minimal condition $\min_W ||W - W_t||^2$ on each update. In both cases local changes are shown to provide global improvement, e.g. {\color{red} \cite{kivinen2004online} show ...} and \cite{crammer2006online} show the total number of updates to be bounded in the linearly-separable case.

%,  The loss function combines the feedback signal $f_t$ with the actual response $\tilde{y}_t$ and the current classifier's parameters $W_t$ to provide additional information about the direction to be followed for future classification improvement (i.e. risk minimization). 
%The loss thus relies on a combiation of \emph{internal} information (current classifier's parameters, current choice) and \emph{external} information (input vector, feedback). On contrary to the offline setting, the classifier's current value $W_t$ plays a significant role in defining the future direction. 

Different loss function and margin constraints can be defined depending on task and feedback characteristics. 
Under the multiclass setting, two principal margin constraint schemes can be set up, namely the \emph{normative} margin and the \emph{relative} margin. In the \emph{normative} margin case, the classifier is expected to provide a response that overtakes in norm a reference value $a$. A typical normative margin setup is the one-vs-all (OVA) setup, having  $a = 1$ for reference,  i.e. $$\forall k \left\{
\begin{array}{l}
\langle w^k, x \rangle \geq 1 \text{ if } y = k \\
\langle w^k, x \rangle \leq -1 \text{ if }y \neq k.
\end{array}
\right. 
$$
and a corresponding normative multiclass \emph{hinge loss} is:  
$$ l(x,W,y) = \sum_{k=1}^K \left[1 + (1 - 2 \delta(y,k)) \langle w^k,x \rangle\right]_+
$$
with $\delta(i,j) =1$ if $i = j$ and 0 elsewhere, and $[u]_+$ equal to $u$ if $u\geq 0$ and 0 elsewhere.
Under  a \emph{relative} margin setup (see \cite{crammer2003ultraconservative}), compliant with a ``Winner-Takes-All'' approach (WTA), there is no absolute reference value but instead a distance reference, so that the linear score of the class-compliant separatrix $\langle w^y, x\rangle$ is expected to overtake  the other linear scores by at least $a$, i.e. (taking $a=1$)
$$ \langle w^y, x \rangle \geq 1 + \max_{k \neq y} \langle w^k, x \rangle  $$
and a corresponding relative multiclass \emph{hinge loss} is:
$$ l (x,W,y) =  \left[ 1 +  \langle w^y, x \rangle - \max_{k \neq y} \langle w^k, x\rangle\right]_+$$

The normative setup is clearly overconstrained regarding the classification task (see \cite{crammer2003ultraconservative}), but in counterpart provides mapping-independence among the different classes.

%{\color{red} Except for the multiclass perceptron (see  \cite{duda1973pattern} and \cite{freund1999large}), which does not rely on 
%Not all multiclass online learning algorithms fall either into the normative or the relative category. 
% while not margin aware, is a typical normative multiclass approach for it  imposes a positive fit with class-compliant exemplars, and a negative fit with class-negative exemplars.  
%On contrary, the multiclass passive-agressive scheme proposed in \cite{crammer2006online} obeys to a relative approach to online multiclass learning.   }



{\color{green} Notion de conservatisme vs exhausivité (Anlauf) et progressisme (Kivinen).}



%\section{Online multiclass learning}




%Les problèmes de bandit simples se généralisent au cas des bandits dits contextuels \cite{langford2008epoch}. 

%Contextual bandit problems  provide sensibly richer universes with



%Un problème de bandit contextuel est également défini par un univers et un apprenant. . Autrement dit, à chaque contexte distinct correspond une distribution de gains distincte sur l'ensemble des $K$ bras. On peut rajouter une hypothèse supplémentaire selon laquelle à des contextes proches correspondent des distributions proches. 

\section{Our approach}


The one-bit feedback provides an asymmetrical situation in which prediction miss provide a poor classification information (eliminates one response out of $K$ possibilities) while prediction hit provide a rich classification information (eliminates $K-1$ responses out of $K$ possibilities). 
In the initial situation where little information is provided, a learner is suggested to try labels at random until a label hit is obtained. This is not necessary the case in the subsequent steps where enough information has been gathered, providing better classification accuracy. 
This intuition needs to be carefully considered, for the effective information (or innovation) gathered is the difference between prediction and actual response. Mistakes (predicting a hit and obtaining a miss, or vice-versa) are  more informative than actual classification achievements (this point is e.g. exemplified  in the perceptron algorithm where only mistakes provide classifier update). The choice of an exploration strategy thus depends on the focus put either on gathering information or optimizing rewards. 

Model-based setups (see \cite{lai1985asymptotically}, \cite{auer2003nonstochastic}, \cite{crammer2013multiclass}) generally use model information (number of visits, hits variance, etc.) to optimize rewards. 
Discriminant-based setups (see \cite{kakade2008efficient}, \cite{zhong2015esann}) put a greater focus on classification information gathering through  
%$\varepsilon$-greedy policies giving a constant budget ($1-\varepsilon + \frac{\varepsilon}{K}$) to the actual prediction and $\frac{\varepsilon}{K}$ to the other choices 
a constant exploration rate $\varepsilon$ that balances hits and misses prediction against actual hits and misses
(see eq. (\ref{eq:kaka-policy})).
Unpredicted hits play a particular role for they are given a higher weight in the update, reflecting a "prediction surprise" (see for instance eq. (\ref{eq:kaka-update})). This information-weigted update is however done at the risk of an increased variance of the update sequence and of course of a linear regret.
%, where pure exploration ($\varepsilon = 1$ i.e. uniform prediction) and pure greediness ($\varepsilon = 0$, i.e. hit prediction only) are special cases.


%address the asymmetry problem
%The unpredicted hits obtained in the second case 
%
%is systematized in the form of an \textit{exploration policy} that sets the balance between exploration and exploitation.     

%The consequence is that on average little classification information is obtained in the first steps of the learning procedure, slowering convergence rates in proportion. 

%A one-bit feedback approach mainly differs from the supervised case by considering with more care exploration requirements.  

Moreover, the computational efficiency of the perceptron (\cite{rosenblatt1958perceptron}, as well as the SVM \cite{vapnik1998statistical},  critically relies on their sparsity, i.e. their capability to separate the example  set between relevant and irrelevant observation vectors, given the classification task. Fewer vectors in a classifier provide better generalization capabilities and participate in regularization.
On contrary to their supervised counterpart, information-weighted updates such as Banditron (\cite{kakade2008efficient}) and PAB (\cite{zhong2015esann}) show a dense classifier update, loosing the Kernel-extension capability (at reasonable computational cost). 

The ability to choose example vectors is however  a critical property of the discriminant approach that needs to be conserved in the partially supervised setup considered here. 
Taking inspiration from \cite{crammer2006online}, we consider a specific one-bit feedback loss function, under one-vs-all multiclass classification framework (see eq. (\ref{eq:BPA-loss})). A rapid inspection shows that label misses provide enough information to update one class separatrix, while label hits give the capability to update $K$ separatrices. For sparsity reasons however, and following the "ultraconservative" approach proposed by \cite{crammer2003ultraconservative}, only the error-set is considered for the classifier update, i.e. only a single separatrix may be updated under non-zero loss condition.

{\color{blue}
La perte instantanée est ici définie comme :
$$l_t = [1 + (1 - 2 g_t) \langle W_{t-1}, X_t^{\tilde{y}_t}\rangle]_+$$
autrement dit~:
\begin{itemize}
	\item[] $l_t = [1 - \langle W_{t-1}, X_t^{\tilde{y}_t}\rangle]_+$ si $g_t=1$;
	\item[] $l_t = [1 + \langle W_{t-1}, X_t^{\tilde{y}_t}\rangle]_+$ sinon
\end{itemize}
Dans le premier cas (le choix est correct), la perte décroît avec $\langle W_{t-1}, X_t^{\tilde{y}_t}\rangle$ jusqu'à 0. Au contraire, dans le second cas (choix incorrect), la perte augmente avec le produit $\langle W_{t-1}, X_t^{\tilde{y}_t}\rangle$.
Minimiser la perte revient donc à faire croître le produit $\langle W_{t-1}, X_t^{\tilde{y}_t}\rangle$ dans le premier cas, et à faire décroître le produit $\langle W_{t-1}, X_t^{\tilde{y}_t}\rangle$  dans le second cas.}

Contrarily to the information-based approach, the weight attributed to each relevant example vector relies on a quadratic update minimization objective under a class-accuracy linear constraint
$ l_t $.
The weight update is the solution of ~:
$$W_{t} = \arg \min_W \frac{1}{2} \| W - W_{t-1}\|^2 + C \xi^2 \hbox{ s.t. } l_t \leq \xi$$
where $C$ is an optional misclassification stiffness parameter.


%The use of a relative multiclass hinge loss {\color{red}(see eq. XXX)} provides the following update~:
%$$W_{t} =  W_{t-1} + \frac{l_t}{2\|x_t\|^2 + \frac{1}{2C}} (X_t^{y_t} - X_t^{s_t})$$
%where $s_t = \arg\max_{s \neq y_t} \langle W_{t-1}, X_t^s \rangle$.
%Combined with a variance reduction technique, this update term was used as a baseline in \cite{zhong2015esann} to address the one-bit feedback, showing effective convergence rates in large-scale real-world problems. {\color{red} However, like the Banditron and like most model-based setups, this initial approach suffers from a lack of parsimony, i.e. implements a }


In line with \cite{kakade2008efficient},  we adapted the multiclass passive-agressive update to the one-bit feedback case  and show in practice faster convergence rates than the Banditron . 
The two setups however 

Dans le cas linéairement séparable, l'apprentissage en ligne d'un perceptron multi-classes peut être significativement accéléré en utilisant les principes de l'optimisation quadratique tels que proposés par Vapnik \cite{vapnik1998statistical}. Dans un article récent, Crammer et al \cite{crammer2006online} proposent une méthode d'apprentissage en ligne supervisée reposant sur l'optimisation locale de la fonction de perte ``hinge loss'' définie dans le cadre multi-classe comme : 

où $[.]_+$ retourne l'identité pour les valeurs positives et 0 pour les valeurs négatives. 

La résolution du problème :

où $C$ correspond au paramètre de raideur, conduit à la mise à jour~:
. Crammer montre que dans le cas linéairement séparable, la somme des pertes au carré est bornée (ce qui est comparable au perceptron). Mais, de manière plus intéressante, il montre également que le regret est  borné en $O(\sqrt{(T)})$ dans le cas non séparable.

  


\subsection{Principe}
L'approche adoptée dans le cadre de cet article est une généralisation assez directe des principes d'optimisation quadratique en ligne proposés par Crammer au cas des bandits contextuels. Cette extension nécessite de définir une fonction de perte spécifique. 

Soit un classifieur linéaire défini à l'essai $t$ par le jeu de paramètres $W_{t-1}$. Après avoir observé l'exemplaire $x_t$, le choix $\tilde{y}_t$ repose sur un tirage selon la distribution: $$P(\tilde{Y}=k) = (1 - \varepsilon) \mathbf{1}_{k = \hat{y}} + \frac{\varepsilon}{K} $$ avec $\varepsilon \in [0,1]$ le paramètre d'exploration, et $\hat{y}$ la catégorie correspondant au meilleure score des produits salaires entre entre l'exemplaire et les différentes séparatrices.  

Après avoir émis la proposition $\tilde{y}_t$, l'apprenant reçoit un gain $g_t$ égal à 1 si la catégorie proposée est correcte et 0 sinon, soit $g_t = \mathbf{1}_{\tilde{y}_t=y_t}$. 



\emph{NB : the same objective is assigned in the ``one-vs-all'' scheme in the multi-class classification scheme. It is a non-conservative (normative) objective function. In case the dataset is separable, $\exists U$ such that $\forall t, l_t = 0$  }

La résolution du problème d'optimisation suivant~:
$$W_{t} = \arg \min_W \frac{1}{2} \| W - W_{t-1}\|^2 + C \xi^2 \hbox{ s.t. } l_t \leq \xi$$
où $C$ correspond au paramètre de raideur, conduit à la mise à jour~:
$$W_{t} =  W_{t-1} + \frac{l_t}{\|x_t\|^2 + \frac{1}{2C}} (2g_t - 1) X_t^{\tilde{y}_t}$$

\paragraph{Bandit Passive-Aggressive (BPA)}
%\caption{BPA}
\begin{algorithmic}
	\STATE $\ \ $
	\STATE Parameters:  $\varepsilon$, $C$.
	\STATE Set $W_0 = 0$ (null vector)
	\FOR {each round $t$ = 1,\dots, $T$}
	\STATE Observe $x_t$.
	\STATE Set $\hat{y}_t = \underset{i = 1,\dots,K}{\text{argmax}}\left\langle W_{t-1} ,X_t^i\right\rangle$
	\FORALL {$i \in [1,...,K]$}
	\STATE $p_{i,t}= (1-\varepsilon)\mathbf{1}_{i = \hat{y}_t} + \frac{\varepsilon}{K}$
	\ENDFOR
	\STATE Draw $\tilde{y}_t$ randomly from $p_t = \left(p_{1,t},\dots ,p_{K,t}\right)$.
	\STATE Observe $g_t = \mathbf{1}_{(\tilde{y}_t=y_t)}$.
	\STATE Set $l_t = \left[ 1+(1-2g_t)\langle W_{t-1},X_t^{\tilde{y}_t}\rangle\right]_{+}$ 
	\STATE $W_t = W_{t-1} + (2g_t-1)\frac{l_t}{\parallel X_t^{\tilde{y}_t}\parallel^2 + \frac{1}{2C}}\cdot X_t^{\tilde{y}_t}$
	\ENDFOR
\end{algorithmic}


\subsection{Analyse}
Soit $U$ est un classifieur quelconque de l'espace des classifieurs, on note $l_t^{\ast}$ la perte obtenue par ce classifieur à l'instant $t$ lorsque la réponse $\tilde{y}_t$ est produite. On démontre les deux théorèmes suivants (en négligeant la constante de raideur $C$ pour simplifier) :

\begin{theorem}
	\label{theo:BPAT1}
	Let $(x_1,y_1),...,(x_T,y_T)$ be a sequence of separable examples where $x_t \in \mathbb{R}^d$, $y_t\in \{1,...,K\}$ and $\parallel x_t \parallel\leqslant R$ for all t, and $U \in \mathbb{R}^{K\times d}$ such that $ \forall t, l^*_t=0$. Then, the cumulative squared loss of this algorithm is bounded by,
	\begin{equation}
	\sum_{t=1}^{T} l_t^2 \leqslant R^2\cdot \parallel{U}\parallel^2
	\end{equation}
\end{theorem}

\begin{proof}
	Define $\Delta_t$ to be:
	\[\Delta_t = \parallel{W_{t-1}-U}\parallel^2-\parallel{W_t-U}\parallel^2\]
	By summing $\Delta_t$ over all $t$ from 1 to $T$,  $\sum_t \Delta_t$ is shown to be a telescopic sum which collapses to
	\begin{align}
	\sum_{t=1}^{T}\Delta_t &= \sum_{t=1}^{T} \left( \parallel{W_{t-1} - U}\parallel^2-\parallel{W_t - U}\parallel^2 \right)\nonumber\\ 
	&= \parallel{W_0 - U}\parallel^2-\parallel{W_t-U}\parallel^2\nonumber
	\end{align}	
	By the initialization of $W_0 = \vec{0}$, 
	\begin{equation}
	\label{equa:delta}
	\sum_{t=1}^{T}\Delta_t = \parallel{U}\parallel^2 - \parallel{W_t-U}\parallel^2 \leqslant \parallel{U}\parallel^2 
	\end{equation}
	
	Using the definition of update : %in Eq.\ref{eq:,
	\begin{align}
	\Delta_t = -2\left\langle (W_{t-1} - U), (2g_t-1)\frac{l_t}{\parallel{x_t}\parallel^2}X_t^{\tilde{y}_t}\right\rangle 
	- \left\| \frac{l_t}{\parallel{x_t}\parallel^2}X_t^{\tilde{y}_t}\right\|^2
	\nonumber
	\end{align}
	%With    and   ,
	So, taking~:
	\begin{itemize}
		\item[] $l_t = [1+(1-2g_t)\cdot\langle W_{t-1},X_t^{\tilde{y}_t}\rangle]_+$
		\item[] $l_t^{\ast} = [1+(1-2g_t)\cdot\langle U,X_t^{\tilde{y}_t}\rangle]_+$
		\item[] $\parallel{X_t^{\tilde{y}_t}}\parallel = \parallel x_t\parallel$
	\end{itemize}
	it comes:
	\begin{align}
	\Delta_t =& 2l_t\frac{(1-2g_t)\langle W_{t-1}, X_t^{\tilde{y}_t}\rangle - (1-2g_t)\langle U, X_t^{\tilde{y}_t}\rangle}{\|x_t\|^2}
	-\frac{l_t^2}{\parallel{x_t}\parallel^2}\nonumber
	\end{align}
	Noting that $\Delta_t = 0$ when $l_t = 0$, and $l^*_t \geq  1+(1-2g_t)\cdot\langle U,X_t^{\tilde{y}_t}\rangle$, it comes : 
	\begin{align}
	\Delta_t\geqslant& 2l_t\frac{l_t - l_t^{\ast}}{\parallel{x_t}\parallel^2}-\frac{l_t^2}{\parallel{x_t}\parallel^2}\nonumber\\
	=& \frac{l_t^2-2l_t l_t^{\ast}}{\parallel x_t\parallel^2}\nonumber
	\end{align}
	If all examples are separable, $\exists U$ such that $\forall t \in [1,...,T]$ , $l_t^{\ast} = 0$ ,
	%, following the Eq.~\ref{sumDelta},
	
	\[\Rightarrow \parallel{U}\parallel^2 \geqslant \sum_{t=1}^{T}\Delta_t \geqslant \sum_{t=1}^{T}  \frac{l_t^2}{\parallel{x_t}\parallel^2}
	\geqslant 
	\sum_{t=1}^{T}  \frac{l_t^2}{R^2}
	\]
	\[\Rightarrow\sum_{t=1}^{T} l_t^2 \leqslant R^2 \cdot \parallel{U}\parallel^2\]
\end{proof}
\begin{theorem}
	\label{theo:BPAT2}
	Let $(x_1,y_1),...,(x_T,y_T) $ be a sequence of examples where  $x_t\in \mathbb{R}^d$, $y_t \in \{1,...,K\}$ and $\parallel{x_t}\parallel \leqslant R$ for all t. Then for any  $U \in \mathbb{R}^{K\times d}$, the cumulative squared loss of this algorithm is bounded by:
	\[\sum_{t=1}^{T}l_t^2 \leqslant \left(R\parallel{U}\parallel+2 \sqrt{\sum_{t=1}^{T}(l_t^{\ast})^2}\right)^2 \]
\end{theorem}
\begin{proof}
	By the proof of Theorem \ref{theo:BPAT1}, 
	\[\sum_{t=1}^{T}l_t^2 \leqslant R^2\cdot \parallel{U}\parallel^2 + 2\sum_{t=1}^{T}l_t l_t^{\ast}\]
	To upper bound the right side of the above inequality, we denote $a_t = \sqrt{\sum_{t=1}^{T}l_t^2}$ and $b_t = \sqrt{\sum_{t=1}^{T}(l_t^{\ast})^2}$, 
	\begin{align}
	2(a_tb_t)^2-2(\sum_{t=1}^{T}l_tl_t^{\ast})^2 =& \sum_{i=1}^{T}\sum_{j=1}^{T}l_i^2(l_j^{\ast})^2+\sum_{i=1}^{T}\sum_{j=1}^{T}l_j^2(l_i^{\ast})^2 \nonumber\\
	&- 2\sum_{i=1}^{T}\sum_{j=1}^{T}l_il_jl_i^{\ast}l_j^{\ast}\nonumber\\
	=& \sum_{i=1}^{T}\sum_{j=1}^{T}(l_il_j^{\ast}-l_jl_i^{\ast})^2 \geqslant 0 \nonumber
	\end{align}
	
	\begin{align}
	\sum_{t=1}^{T}l_t^2 \leqslant R^2 \cdot \parallel{U}\parallel^2+2\sum_{t=1}^{T}l_tl_t^{\ast}\leqslant R^2 \cdot \parallel{U}\parallel^2+2a_tb_t\nonumber
	\end{align}
	then considering:
	\[a_t^2 -2 a_tb_t+b_t^2\leqslant R^2\parallel{U}\parallel^2+b_t^2\]
	%the largest possible $a_t$ respecting the inequality is $b_t+\sqrt{R^2\parallel{U}\parallel^2+b_t^2}$
	we obtain :
	\[a_t \leqslant b_t+\sqrt{R^2\parallel{U}\parallel^2+b_t^2}\]
	and using the fact that $\sqrt{a+b}\leqslant \sqrt{a}+\sqrt{b}$,
	\[a_t \leqslant R\parallel{U}\parallel+2 b_t\]
	so that :
	\[\sum_{t=1}^{T}l_t^2 \leqslant \left(R\parallel{U}\parallel+2 \sqrt{\sum_{t=1}^{T}(l_t^{\ast})^2}\right)^2 \]
\end{proof}

%Pour les affiliations, vous pouvez utiliser
%\href{http://ctan.org/pkg/authblk}{le paquet \texttt{authblk}}.

Les bornes obtenues ici sont comparables  à celles obtenues par Crammer dans le cadre de la classification supervisée  \cite{crammer2006online}. En particulier, comme dans le cas de l'algorithme ``passif-agressif'' de Crammer, on peut s'attendre à un regret de l'ordre de $O(\sqrt{T})$ dans le cas non-linéairement séparable. Il est donc remarquable de constater que les garanties de convergences de notre algorithme sont les mêmes que dans le cas supervisé, étant donnée la moindre information utilisée et le caractère stochastique de la fonction de décision. On notera néanmoins le caractère plus ``faible'' de la fonction de perte utilisée, de sorte que nos résultats ne garantissent pas que la bonne réponse sera atteinte, mais seulement que la mauvaise réponse ne sera pas atteinte.

The fact we obain the same bounds as \cite{crammer2006online} indicates that additional degrees of freedom not to be considered.   

\subsection{Extensions}
Le plongement des données d'entrée dans des espaces de Hilbert à noyaux reproduisants (RKHS) permet d'étendre notre approche aux jeux de données non linéairement séparables. 

Soit $\mathcal{H}$ un espace de Hilbert à noyaux reproduisants dont le produit scalaire est défini à l'aide de la fonction noyau $\mathcal{K}$. On note $\mathcal{K}(x,.)$ la projection de l'exemplaire $x$ dans $\mathcal{H}$, avec $\forall f \in \mathcal{H}, \langle f,\mathcal{K}(x,.)\rangle_\mathcal{H} = f(x) $.


Le classifieur est dans ce cadre défini comme un ensemble de fonctions : $\mathcal{F} = \{f^{(1)}, ..., f^{(K)}\}$ avec :
$$\hat{y} = \arg \max_k f^{(k)}(x)$$

Soit $\mathcal{F}_0=\{0, ..., 0\}$ le classifieur initial. A chaque essai, il est mis à jour selon la règle définie précédemment, soit à l'instant $t$~:
$$\mathcal{F}_t = \{f^{(1)}_t, ..., f^{(K)}_t\}$$
$$\forall k, f^{(k)}_t = \sum_{t^\prime = 1} ^t  \frac {\mathbf{1}_{k=\tilde{y}_{t^\prime}}l_{t^\prime}}{\mathcal{K}(x_{t^\prime},x_{t^\prime})+\frac{1}{2C}} (2g_{t^\prime} - 1)\mathcal{K}(x_{t^\prime},.)$$
$$ l_t = [1 + (1-2g_t) f_{t-1}^{(\tilde{y}_t)}(x_t)]_+$$

En notant $\alpha_t^{(k)} = \frac {\mathbf{1}_{k=\tilde{y}_{t}}l_{t}}{\mathcal{K}(x_{t},x_{t})+\frac{1}{2C}} (2g_{t} - 1)$, il vient :
$$\forall k, f^{(k)}_t = \sum_{t^\prime = 1} ^t \alpha_{t^\prime}^{(k)} \mathcal{K}(x_{t^\prime},.)$$

Autrement dit chaque séparatrice $f^{(k)}$ est définie par un ensemble de vecteurs supports ($x_{t_1^{(k)}}$, ..., $x_{t_i^{(k)}}$, ...  ) tels que $\forall i, \alpha_{t_i^{(k)}}^{(k)} \neq 0$.


Le nombre de vecteurs supports augmente de 1 chaque fois que la perte est non nulle. De par le théorème 1, nous savons que ce nombre est borné dès lors que les données sont séparables dans l'espace de redescription. 

Afin de contrôler plus strictement le nombre de vecteurs supports, nous considérons une approche alternative basée sur la minimisation du risque régularisé par descente de gradient comme proposé dans \cite{kivinen2004online}~:
$$R(\mathcal{F}) = \mathbb{E}\left[ l_t + \frac{\lambda}{2}\|\mathcal{F}\|^2_\mathcal{H}\right]$$
On obtient dans ce cas la règle de mise à jour~:
$$f_t^{(k)}=\left\{
\begin{array}{l}
(1-\eta\lambda) f_{t-1}^{(k)} + \eta (2g_t-1) \mathcal{K}(x_t,.) \text{ si } k=\tilde{y}_t\\
f_{t-1}^{(k)} \text{ sinon}
\end{array}
\right.
$$
avec $\eta$ paramètre d'apprentissage et $\lambda$ paramètre de régularisation. En notant 
$$\sigma_t^{(k)} = \sum_{t^\prime=1}^t
\mathbf{1}_{\tilde{y}_{t^\prime}=k}$$
on obtient dans ce cas:
$$\forall k, f^{(k)}_t = \sum_{t^\prime = 1} ^t \alpha_{t^\prime}^{(k)} \mathcal{K}(x_{t^\prime},.)$$
et
$$\alpha_{t^\prime}^{(k)} = \mathbf{1}_{\tilde{y}_{t^\prime}=k}(1 - \eta \lambda)^{\sigma_t^{(k)} - \sigma_{t^\prime}^{(k)}-1}  \eta (2g_{t^\prime}-1)$$

Comme proposé par Kivinen, il suffit alors de tronquer la séquence de vecteurs supports servant à définir la séparatrice en éliminant les termes les plus faibles, typiquement les exemplaires $t^\prime$ tels que $\sigma_{t^\prime}^{(k)} > H$, avec $H$ horizon définissant le nombre maximal de vecteurs supports conservés par séparatrice. Cet artifice borne naturellement le nombre de vecteurs supports du classifieur, en ne considérant que les exemplaires les plus récemment observés (les exemplaires les plus anciens sont ``oubliés'').  Kivinen montre que l'erreur de troncature décroît exponentiellement avec la taille de l'horizon choisi. 




%Et évidemment, vous ajoutez ensuite les paquets que vous voulez
%utiliser, les macros les définitions de théorèmes etc... Nous
%recommandons le paquet \texttt{hyperref} puisque les documents
%\texttt{PDF} seront en ligne si vous avez donné

\section{Experiments}

\subsection{High-dimensional datassets}
\label{subsec:BPAE}
Here, we evaluate the algorithms over two synthetic and three real world data sets. Their characteristics are summarized in Table~\ref{table:mce}.

\begin{table}[h]
	\caption{Summary of the three high-dimensional datasets, including the numbers of instances, features, labels and whether the number of examples in each class are balanced.}
	\label{table:mce}
	\begin{center}
		\begin{tabular}{l l l l l}
			{\bf Dataset}  & {\bf Instances} & {\bf Features} & {\bf Labels}& {\bf Balanced}\\
			\hline
			SynSep & $10^5$ 	& 400 	& 9 & Y\\
			
			SynNonSep & $10^5$ & 400 	& 9 & Y\\
			
			RCV1-v2  & $10^5$ 	& 47236 	& 53 & N\\
			
			%Letter 	&$2*10^4$	&16	&26	&N\\
			
			%Pen-Based &$1.32*10^4$	&16	&10	&N\\
		\end{tabular}
	\end{center}
\end{table}

\textbf{Data sets}:
The first data set, denoted by SynSep,  is a 9-class, 400-dimensional synthetic data set of size $10^5$. More details about the method to generate this data set can be found in \cite{kakade2008efficient}. The SynSep  idea is to have a simple simulation of generating a text document. The coordinates represent different words in a small vocabulary of size $400$. We ensure that SynSep is linearly separable. 

The second data set, denoted by SynNonSep, is constructed  the same way as  SynSep except that a 5\% label noise is introduced, which makes the data set non-separable. 

The third data set is collected from the Reuters RCV1-v2 collection\cite{David04RCV}. The original data set is composed by multi-label instances. So we make some preprocessing likes \cite{RB08a}. First, its label hierarchy is reorganized by mapping the data set to the second level of RCV1 topic hierarchy. The documents that have labels of the third or forth level only are mapped to their parent category of the second level; Second, all multi-labelled instances have been removed. This RCV1-v2 is a 53-class,  47236-dimensional real data set of size $10^5$. 

%The fourth and fifth data sets are collected from \cite{letter26SC,number10SC}. The fourth data set is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20000 unique stimuli. Each stimuli was converted into 16 primitive numerical attributes (statistical moments and edge counts). It forms a 26-class, 16-dimensional real data set of size $20000$. The fifth data set is a digit data base made by collecting 250 samples from 44 writers, using only (x,y) coordinate information represented as constant length feature vectors, which were resampled to 8 points per digit (therefore the data set contains 8 points $\times$ 2 coordinates = 16 features). This one is a 10-class, 16-dimensional real data set of size $10992$.

\textbf{Results}
Figures \ref{pic:BPASS} and~\ref{pic:BPASNS} show the experimental results on two synthetic data sets. For SynSep, a separable linear data set, all algorithms except Banditron obtain a good performance; with the non-separable SynNonSep data, Confidit and BPA outperform the other algorithms, even the supervised algorithms.  To different datasets, the parameters of different algorithms refer to Table~\ref{table:bpa}.
\begin{table}[h]
	\caption{The summary of algorithm parameters for different datasets. P. denotes Perceptron, PA is Passive-Aggressive online algorithm, B. is Banditron, C. is Confidit and BPA.}
	\label{table:bpa}
	\begin{center}
		\begin{tabular}{lllllll}
			{\bf Dataset}  & {\bf P.} & {\bf PA } & {\bf B.}& {\bf C.} & {\bf BPA}\\
			\hline
			SynSep & null & $C=0$ & $\varepsilon = 0.014$ &$\eta = 10^3$ & $\varepsilon = 0.4,C = 0$\\
			
			SynNonSep & null & $C=10^{-2}$ & $\varepsilon =0.65$ & $\eta = 10^3$& $\varepsilon = 0.8,C = 10^{-2}$\\
			
			Reuters & null & $C=10^{-2}$ & $\varepsilon =0.4$ & $\eta = 10^2$ & $\varepsilon = 0.2,C = 10^{-2}$\\
			
			%LR(26 letters) & null &  $C=0.1$ & $\varepsilon = 0.2$& $\eta=10^2$ & $\varepsilon = 0.8,C= 1$ \\
			
			%LR(10 numbers) & null & $C=0.1$ & $\varepsilon= 0.4$& $\eta = 10$ & $\varepsilon = 0.6,C=1$\\
			
		\end{tabular}
	\end{center}
\end{table}

%\textcolor{red}{OK-- Il manque les valeurs des paramètres pour les différents algorithmes (faire un tableau comme dans la partie précédente).}

Figure~\ref{pic:BPARCV} %, ~\ref{pic:BPALR10} and~\ref{pic:BPALR26} 
presents the result on the real dataset. With this dataset, the supervised algorithms, despite their competitive advantage with respect to the ones with bandit feedback, do not significantly depart from BPA and Confidit, with classification results that clearly outperform Banditron. While having a lower computational complexity, BPA approach is even found to outperform Confidit in the most challenging situation, i.e. the high-dimensional case with a large number of classes (RCV1-v2 data set).

The $\epsilon$ parameter represents the exploration rate in Banditron and BPA algorithms. We compare on Figure 3 the average error rates obtained on the two algorithms for different values of $\epsilon$ on the different data sets. In contrast with Banditron, BPA shows that $\epsilon$ has a very little influence on the final error rate, indicating a capability to deal with small exploration rates.



\begin{figure}[h!]
	
	\centerline{
		\includegraphics[scale = 0.4]{figs/SynSep.eps}
	}
	\caption{Cumulative Errors on the synthetic data set of  SynSep.}
	\label{pic:BPASS}
\end{figure}
\begin{figure}[h!]
	
	\centerline{
		\includegraphics[scale = 0.4]{figs/SynNonSep.eps}
	}
	\caption{Cumulative Errors on the synthetic data set of SynNonSep.}
	\label{pic:BPASNS}
\end{figure}
\begin{figure}[h!]
	\centerline{
		\includegraphics[scale = 0.4]{figs/RCV1_v2_53class.eps}}
	\caption{Cumulative Errors  on the real data set of RCV1-v2 (53 classes).}
	\label{pic:BPARCV}
\end{figure}

\begin{figure}[h!]
	\centerline{
		\includegraphics[scale = 0.4]{figs/10LR.eps}}
	\caption{Cumulative Errors on the real data set of Letter Recognition (10 numbers).}
	\label{pic:BPALR10}
\end{figure}

\begin{figure}[h!]
	\centerline{
		\includegraphics[scale = 0.4]{figs/26LR.eps}}
	\caption{Cumulative Errors  on the real data set of Letter Recognition (26 Letters).}
	\label{pic:BPALR26}
\end{figure}

\begin{figure}[h!]
	\centerline{
		\includegraphics[scale = 0.4]{figs/SynNonSep_gamma.eps}}
	\caption{Average error of Banditron and BPA for parameter's value $\epsilon$ on the data set of SynNonSep. }
	\label{pic:BPASNSerr}
\end{figure}

\begin{figure}[h!]
	\centerline{
		\includegraphics[scale = 0.4]{figs/Reuters_gamma.eps}}
	\caption{Average error of Banditron and BPA for parameter's value $\epsilon$ on the data set of Reuters.}
	\label{pic:BPARCVerr}
\end{figure}

\begin{figure}[h!]
	\centerline{
		\includegraphics[scale = 0.4]{figs/10LR_gamma.eps}}
	\caption{Average error of Banditron and BPA for parameter's value $\epsilon$ on the data set of Letter Recognition.}
	\label{pic:BPALRerr}
\end{figure}


\subsection{Non-linearly separable datasets}

In this section, we take two datasets to evaluate and analyze the effect of these algorithm in Reproducing Kernel Hilbert Space.

\vspace{1.5ex}
\textbf{Data description}
The first dataset denoted by Pendigits, is a real data and created by E.Alpaydin and Fevzi.Alimoglu \cite{alimoglu1996combining,Alimoglu96methodsof}. 
It  collected 250samples from 44 writers. All writers are asked to write 250 digits in random order inside boxes of 500 by 500 tablet pixel resolution. 
Here, the dataset is part of original one. It contains 7494 instances, 16 features and 10 classes. 

The second dataset denoted by `Segment'\cite{Lichman:2013}. This dataset contains 2310 instances, all of them were drawn randomly from a database of 7 outdoor images. The images were handsegmented to create a clasification for every pixel. Each instance is a $3\times 3$ region. It's a real dataset, with 19 features and 7 classes. More details could be referred to the data site ``UCI''.

\vspace{1.5ex}
%\textcolor{red}{OK-- Il faut donner la formule des noyaux lineaire et Laplace}
\textbf{Algorithm}
Here, we take algorithms Banditron (in RKHS), KBPA and KSGD to compare. In order to perform the effect of RKHS, we choose KBPA in linear model as the reference object and choose \textbf{Laplace} for the kernel function. Its form looks like the following formulate.
\[K_{Laplace}(x,y) = \exp{\left(-\frac{\parallel{x-y}\parallel}{\sigma}\right)}\]
So, all participant algorithms contains: KBanditron, KBPA (linear), KBPA (Laplace), and KSGD (Laplace). For each dataset, the parameter of kernel function is different. By cross-validation way, we choose $\eta = 1$ of model `Laplace' for dataset Pendigits and $\eta = 10$ for dataset `Segment'. For KSGD, the truncated number is 500 for dataset Pendigits, and 200 for Segment.

\vspace{1.5ex}
\textbf{Result}
We mainly analyze these experiments from the following aspects. 

Average training time for each instance:  we observe the training time of every instance $\{t_1,t_2,\dots,t_n\}$; then divide 100 ordering examples into one group $g_1 = \{t_1,\dots,t_{100}\}$,
$\dots$, $g_i = \{t_{1+100*(i-1)},\dots, t_{100*i}\}$; finally, the average training time for instances of group $g_i$ can be calculated by $\overline{t_i} = \frac{1}{100}\sum_{s=1+100*(i-1)}^{100*i} t_s$. 

Average error rate: $e_i = \sum_{s=1+100*(i-1)}^{100\times i}\mathbf{1}_{\hat{y}_t = y_t}/100$ this measure is calculated by the same way.

Cumulative Errors: calculate the total number of past errors.

In Figure~\ref{pic:PKT}, it gives the result of average training time on based dataset ``Pendigits''.  From this result, the training time of three kernel algorithms increases linearly along with the number of training instances. Only the linear model is stable. From the theoretical perspective, Banditron always adds a new example passively for its support vector. Algorithm KSGD only adds a new example for its support vector if its classifier makes a bad prediction, otherwise the number of support vector is limited by the truncated parameter. Algorithm KBPA adds a new example for its support vector if and only if its predicted loss not equals to zero. So its number of support vector will increase all the time until it can make good prediction with no loss.

In Figure~\ref{pic:PKM} and Figure~\ref{pic:PKCM}, accumulative errors of algorithm KBPA firstly tend to a stable, others still increase linearly. That is because KBPA accumulates all good support vectors, KSGD only accumulates several recent support vectors and Kernel Banditron always accumulates new instance as negative support vector.

In Figure~\ref{pic:SKT}, it is about the average training time on dataset ``Segment''.  The training time of Kernel Banditron still increases linearly, while the training time of KSGD and KBPA are as stable as linear model after a small period of increasing linearly. KSGD reaches the limited number of support vector, and KBPA quickly gets enough support vectors to make a good prediction. It could show that this dataset is separable. 

In Figure~\ref{pic:SKM} and Figure~\ref{pic:SKCM}, we can observe that KBPA and KSGD performed obviously better than the other two.  Two kernel algorithms have ability to solve non-linear classification with Bandit Feedback. Considering the scale of classifier, we can use more efficient algorithm KBPA if dataset is separable, otherwise we use KSGD.

\begin{figure}[h!]
	\centerline{
		\includegraphics[scale = 0.4]{figs/Pendigits_kernel_T.png}
	}
	\caption{Average training time for each instance of Data Pendigits.}
	\label{pic:PKT}
\end{figure}

\begin{figure}[h!]
	\centerline{
		\includegraphics[scale = 0.4]{figs/Pendigits_kernel_M.png}}
	\caption{Average error rate for each instance of Data Pendigits}
	\label{pic:PKM}
\end{figure}

\begin{figure}[h!]
	\centerline{
		\includegraphics[scale = 0.4]{figs/Pendigits_kernel_CM.png}}
	\caption{Cumulative Errors of Data Pendigits}
	\label{pic:PKCM}
\end{figure}

%\begin{figure}[h!]
%\label{pic:PKR}
%\centerline{
%\includegraphics[scale = 0.4]{fig05/mc/Pendigits_kernel_R.png}}
%\caption{Cumulative loss of Data Pendigits}
%\end{figure}

\begin{figure}[h!]
	\centerline{
		\includegraphics[scale = 0.4]{figs/Segment_kernel_T.png}}
	\caption{Average training time for each instance of Data Segment.}
	\label{pic:SKT}
\end{figure}

\begin{figure}[h!]
	\centerline{
		\includegraphics[scale = 0.4]{figs/Segment_kernel_M.png}}
	\caption{Average error rate for each instance of Data Segment}
	\label{pic:SKM}
\end{figure}

\begin{figure}[h!]
	\centerline{
		\includegraphics[scale = 0.4]{figs/Segment_kernel_CM.png}}
	\caption{Cumulative Errors of Data Segment}
	\label{pic:SKCM}
\end{figure}

%\begin{figure}[h!]
%\label{pic:SKR}
%\centerline{
%\includegraphics[scale = 0.4]{fig05/mc/Segment_kernel_R.png}}
%\caption{Cumulative loss of Data Segment}
%\end{figure}
%\subsection{Conclusion}
\section{Conclusion}
\label{sec:conclusion}
{Conclusion}
\label{subsec:BPAC}

We proposed a novel algorithm for online multiclass classification with bandit feedback. By the advantage of PA max-margin principle, BPA appears effective to address the bandit online learning setting. Its main advantage is its linear complexity in space that allows to deal with high dimensional data sets and a large number of classes, on the contrary to second-order methods. The practicability of this algorithm is verified theoretically by showing a competitive loss bound.

Moreover, experimental evaluation shows that BPA performs better than other algorithms on  real datasets, even better than the algorithms with full feedback on the data sets non-separable.

%In the next section, we will take BPA to deal with non-linear data sets  by combining the Kernel method. 



%Reading :
%Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation Ohad Shamir (NIPS’14)
%Many machine learning approaches are characterized by information constraints on how they interact with the training data. These include 
%memory and sequential access constraints (e.g. fast first-order methods to solve stochastic optimization problems); 
%communication constraints (e.g. distributed learning); 
%partial access to the underlying data (e.g. missing features and multi-armed bandits) 
%algorithm with small memory footprint
%The standard implementation of many common learning tasks requires memory which is super-linear in the data dimension
%The need for fast and scalable learning algorithms has popularised the use of online algorithms, which work by sequentially going over the training data, and incrementally updating a (usually small) state vector
%There has also been considerable interest in online learning with partial information, where the learner only gets partial feedback on his performance. This has been used to model various problems in web advertising, routing and multiclass learning. Perhaps the most well-known case is the multi-armed bandits problem with many other variants being developed, such as contextual bandits, combinatorial bandits, and more general models such as partial monitoring [Bubeck, Cesa-Bianchi]
%sequential decisions


%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\bibliographystyle{elsarticle-harv} 
\bibliography{main}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

% %\begin{thebibliography}{00}

%% \bibitem[Author(year)]{label}
%% Text of bibliographic item

%% \bibitem[ ()]{}

% %\end{thebibliography}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.
